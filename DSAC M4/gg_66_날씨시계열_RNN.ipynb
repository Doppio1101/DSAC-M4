{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gg_66_날씨시계열_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snxLLdjAiHpP"
      },
      "source": [
        "# 시계열 분석\n",
        "- 온도, 습도, 기압, 풍향 등 14개 데이터 사용, 10분마다 기록\n",
        "- 며칠간의 데이터를 입력하고 24시간 이후의 기온 예측\n",
        "\n",
        "## 드롭아웃, 스태킹, 양방향 순환신경망 사용\n",
        "- 순환 드롭아웃: 과대적합을 방지\n",
        "- 스태킹 순환 층: 네트워크의 표현능력을 향상(계산량은 늘어난다)\n",
        "- 양방향 순환 층: 서로 다른 방향으로 데이터를 입력하여 정확도를 높이고 기억을 오랫동안 유지  \n",
        "\n",
        "## 데이터\n",
        "- 독일 Jena시 막스 프랑크 생물지구화학연구소에서 지상 관측소에서 측정한 데이터 \n",
        "- 2009~2016년 데이터 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l83oJpPakC43"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, os.path, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l-hSHFgkImi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2009e7b-ba60-4e8c-9a72-7002215513b3"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip \\\n",
        "    -O jena_climate.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-04 21:59:46--  https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.76.150\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.76.150|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13565642 (13M) [application/zip]\n",
            "Saving to: ‘jena_climate.zip’\n",
            "\n",
            "jena_climate.zip    100%[===================>]  12.94M  13.8MB/s    in 0.9s    \n",
            "\n",
            "2021-01-04 21:59:47 (13.8 MB/s) - ‘jena_climate.zip’ saved [13565642/13565642]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjdOdrH_TJTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1611f9-0270-42e3-a602-1fbe17e1f47c"
      },
      "source": [
        "!unzip jena_climate.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  jena_climate.zip\n",
            "replace jena_climate_2009_2016.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff_WqZ8FTUOq"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V86WWhHQWS58"
      },
      "source": [
        "# 데이터프레임으로 읽는 경우\n",
        "df = pd.read_csv('jena_climate_2009_2016.csv')\n",
        "df[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SPoUtVbXvpq"
      },
      "source": [
        "x = df.drop('Date Time', axis=1).values\n",
        "x[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_Hbg4MiHpS"
      },
      "source": [
        "# data_dir = './data'\n",
        "# fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
        "\n",
        "# csv 파일을 직접 다루는 경우\n",
        "f = open('jena_climate_2009_2016.csv')\n",
        "data = f.read()\n",
        "f.close()\n",
        "\n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "\n",
        "print(header)\n",
        "print(\"데이터 사이즈: \",len(lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPfdoM8IlHVy"
      },
      "source": [
        "lines[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd_KYTw7iHpX"
      },
      "source": [
        "x = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(',')[1:]]\n",
        "    x[i, :] = values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tisf02qo8-T"
      },
      "source": [
        "x[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_seNumiHpb"
      },
      "source": [
        "temp = x[:, 1]  # 기온만 그려본다\n",
        "plt.plot(range(len(temp)), temp)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX9k72EriHpf"
      },
      "source": [
        "### 10일간 데이터\n",
        "- 샘플은 10분마다 측정\n",
        "- 스케일링 필요 (데이터 속성이 다름)\n",
        "- 하루에 140번 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HvEg4LqiHpg"
      },
      "source": [
        "plt.plot(range(1440), temp[:1440])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88g2u4ZniHpl"
      },
      "source": [
        "## 파라미터 준비\n",
        "\n",
        "\n",
        "* `lookback = 720`, 5일간 데이터를 관찰.\n",
        "* `steps = 6`, 1시간마다 한번 샘플링.\n",
        "* `delay = 144`, 24시간 후를 예측.\n",
        "\n",
        "### 준비할 사항\n",
        "\n",
        "- 데이터 변환 (스케일링): 20만개 훈련 샘플을 사용하여 평균과 표준편차를 계산한다.\n",
        "- 데이터가 수치형이므로 벡터화는 사용하지 않는다\n",
        "- 입력 데이터와 레이블을 공급하기 위한 제너레이터를 작성\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNm4RFVziHpm"
      },
      "source": [
        "mean = x[:200000].mean(axis=0)\n",
        "x -= mean\n",
        "std = x[:200000].std(axis=0)\n",
        "x /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rZpgn-tZ_Rk"
      },
      "source": [
        "std, mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuwgbHiNiHpq"
      },
      "source": [
        "제너레이터는 (samples, targets) 형태의 값을 생성한다.  sample은 입력의 한 배치를 나타낸다.\n",
        "\n",
        "제너레이터를 사용하는 이유는 입력 데이터를 모두 한번에 메모리에 미리 로드하지 않기 위해서이다.\n",
        "\n",
        "아래는 제너레이터 함수의 인자이다:\n",
        "\n",
        "* `data`: 스케링된 입력 데이터\n",
        "* `lookback`: 과거의 몇개의 타임스텝을 사용할지 지정\n",
        "* `delay`: 미래 몇 타임스텝후를 예측할지\n",
        "* `min_index` and `max_index`: 검증 및 테스트 데이터를 구분하는 경계를 표시\n",
        "* `shuffle`: 샘플들은 랜덤하게 취할지를 지정\n",
        "* `batch_size`: 배치 크기\n",
        "* `step`: 샘플 데이터를 취할 타임스텝수 (여기서는 6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHO6lmHhiHpr"
      },
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(\n",
        "                min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows),\n",
        "                           lookback // step,\n",
        "                           data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8XSvlCXiHpu"
      },
      "source": [
        "훈련 데이터는 처음 20만개, 검증 데이터는 다음 10만개, 나머지는 테스트 데이터로 사용한다.\n",
        "SGD를 사용하면 과적합을 피하고, 최적값을 찾아낸 확률이 높아진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsRiVyuJiHpv"
      },
      "source": [
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "\n",
        "train_gen = generator(x,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step, \n",
        "                      batch_size=batch_size)\n",
        "val_gen = generator(x,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "test_gen = generator(x,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback) // batch_size\n",
        "test_steps = (len(x) - 300001 - lookback) // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YbObfmLiHp0"
      },
      "source": [
        "## 기본 성능 모델\n",
        "\n",
        "평균치 예측 등, 가장 기본적인 성능을 내는, 최소 기준의 모델을 말한다. 기계학습을 사용하면 적어도 이보다는 성능이 좋아야 한다.\n",
        "  \n",
        "여기서는 내일의 날씨는 오늘과 같다고 단순히 예측하는 모델을 사용하겠다. 성능 평가로 절대값 오차를 사용(MAE). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqriX86NiHp4"
      },
      "source": [
        "- 아래에 기본 성능 모델의 성능을 계산하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ14NY-ciHp6"
      },
      "source": [
        "def evaluate_naive_method():\n",
        "    batch_maes = []\n",
        "    for step in range(val_steps):\n",
        "        samples, targets = next(val_gen)\n",
        "        preds = samples[:, -1, 1]\n",
        "        mae = np.mean(np.abs(preds - targets))\n",
        "        batch_maes.append(mae)\n",
        "    print(np.mean(batch_maes))\n",
        "    \n",
        "evaluate_naive_method()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8v7_bkdiHp-"
      },
      "source": [
        "오차 값 0.29는 정규화 한 값이므로 표준 편차(8.8525)를 곱하여 온도의 오차를 구할 수 있다. 온도의 평균 오차는  \n",
        "2.57˚C이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV2ubn4ZiHqA"
      },
      "source": [
        "## 간단한 신경망 모델\n",
        "\n",
        "- 여기서는 두개의 전결합망으로 구성된 MLP을 먼저 만들어보겠다. 성능평가는 MAE를 사용한다.회귀문제이므로 마지막 단은 활성화함수를 사용하지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kyvqb1BiHqB"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape=(lookback // step, x.shape[-1])))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrAbpjzkiHqI"
      },
      "source": [
        "훈련과 검증 데이터에 대해서 손실함수 그래프를 그려보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vLXQh6diHqK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb7xgsFqiHqS"
      },
      "source": [
        "앞에서 구한 기본성능 0.29을 얻는 것도 쉽지 않다. 모델로 복잡한 구조를 사용했다고 해서 간단한, 상식적인 모델보다 우수한 성능을 내는 것은 아니다. 모델이 이러한 답을 바로 찾을 수 있다는 보장은 없다. (사람의 직관에 의한 기본 성능을 내는 것도 컴퓨터에게는 어려울 수 있다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuBKIa80iHqU"
      },
      "source": [
        "## 간단한 RNN 모델\n",
        "\n",
        "앞의 MLP는 시간 정보를 활용하지 못하고 있다. 시퀀스 데이터를 그대로 사용하고 이를 반영하는 모델을 만들어야 한다. \n",
        "\n",
        "LSTM을 간략히 구현하는 방식인 GRU(gated recurrent unit)을 사용하겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJTSUXQKiHqW"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape=(None, x.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkB_k03QiHqa"
      },
      "source": [
        "[링크 텍스트](https://)아래 결과를 보면 성능이 나아졌다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK8xGJ2SiHqb"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbcDyCEpiHqf"
      },
      "source": [
        "MAE 성능이 0.265 정도로 조금 개선되었다.(2.35˚C)  - 기본 모델의 성능이 2.57˚C 였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XMiUYegiHqf"
      },
      "source": [
        "## 과대적합 줄이기\n",
        "\n",
        "몇 번의 이포크 후에 성능이 바빠지므로 과대적합된 것을 알 수 있다. 오랫동안 순환층 이전에 드롭아웃을 사용하면 학습에 방해가 된다고 알려졌다. 타임 스텝마다 랜덤하게 드롭아웃 패턴을 바꾸지 않고, 동일한 패턴을 모든 타임 스텝에 적용해야 한다. 드롭아웃을 사용하면 학습시간이 오래걸린다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AntVPUN_iHqg"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.2,\n",
        "                     recurrent_dropout=0.2,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLkEk4NJiHqn"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhlv49kniHqs"
      },
      "source": [
        "과대적합이 발생하지 않았다. 성능은 별로 개선되지는 않았다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhEII381iHqt"
      },
      "source": [
        "## 순환망 쌓기 (Stacking)\n",
        "\n",
        "이제 과대적합이 되기 전까지는 네트워크의 복잡도(용량)을 늘려서 성능을 개선할 수 있다. 스태킹을 하려면 중간 계층에서는 시퀀스 전체를 출력해야 한다. 이를 위해서 return_sequences=True 로 설정해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Max-3YU0iHqv"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.1,\n",
        "                     recurrent_dropout=0.5,\n",
        "                     return_sequences=True,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.GRU(64, activation='relu',\n",
        "                     dropout=0.1, \n",
        "                     recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGeQzk1WiHq1"
      },
      "source": [
        "성능을 보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkH4hVrViHq2"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBrkbUxhiHq7"
      },
      "source": [
        "층을 추가해도 성능이 크게 향상되지 않는 것을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J4HOSCHiHq8"
      },
      "source": [
        "## 양방향 RNN\n",
        "\n",
        "자연어 처리에서 우수한 성능을 낸다. RNN을 두개 사용하고 입력 시퀀스를 역방향으로도 사용한 두가지 학습하여 이를 합친다. 이를 통해 단방향이 놓치지 쉬운 패턴을 찾아낼 수 있다.\n",
        "\n",
        "아래와 같이 '순서를 뒤집는' 유명한 코드를 사용한다.\n",
        "\n",
        "`yield samples[:, ::-1, :], targets`\n",
        "\n",
        "\n",
        "온도 예측의 경우, 성능은 오히려 나빠지는 것을 알 수 있다. - 자연어 처리에서는 성능이 개선되는 경우가 많다 (최근 BERT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI9LLaB4iHq-"
      },
      "source": [
        "def reverse_order_generator(data, lookback, delay, min_index, max_index,\n",
        "                            shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(\n",
        "                min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows),\n",
        "                           lookback // step,\n",
        "                           data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples[:, ::-1, :], targets\n",
        "        \n",
        "train_gen_reverse = reverse_order_generator(\n",
        "    float_data,\n",
        "    lookback=lookback,\n",
        "    delay=delay,\n",
        "    min_index=0,\n",
        "    max_index=200000,\n",
        "    shuffle=True,\n",
        "    step=step, \n",
        "    batch_size=batch_size)\n",
        "val_gen_reverse = reverse_order_generator(\n",
        "    float_data,\n",
        "    lookback=lookback,\n",
        "    delay=delay,\n",
        "    min_index=200001,\n",
        "    max_index=300000,\n",
        "    step=step,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHWX59rTiHrA"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen_reverse,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen_reverse,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27AD67qUiHrF"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}